{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\silus\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Users\\silus\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_learning import fit_svm, cross_validate_convNN\n",
    "from load_and_preprocessing import load_data_and_filter_members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put train data import here after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(526, 175)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "\n",
    "X_train = X_train[y_train != 2]\n",
    "y_train = y_train[y_train !=2]\n",
    "\n",
    "X_train = X_train - np.mean(X_train, axis=0)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 2, samples per class 313, 213\n"
     ]
    }
   ],
   "source": [
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Classes {0}, samples per class {1}, {2}\"\n",
    "      .format(len(classes), counts[0], counts[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_hyperparam = np.logspace(-6,-2,11)\n",
    "use_kernel = 'rbf'\n",
    "k_fold = 5\n",
    "\n",
    "mean_accuracy, variance = fit_svm(X_train, y_train, use_kernel, c_hyperparam, k_fold=k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(15, 5))\n",
    "ax1.plot(mean_accuracy, 'b.-')\n",
    "ax1.set_ylabel('Accuracy', color='b')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(variance, 'g.-')\n",
    "ax2.set_ylabel('training loss', color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Nbr. of councillors, nbr. of votes) before filter: (221, 1320)\n",
      "(Nbr. of councillors, nbr. of votes) after filter: (175, 1320)\n"
     ]
    }
   ],
   "source": [
    "leg='50'\n",
    "from_date = None\n",
    "to_date = None\n",
    "link_cutoff = 0.7\n",
    "data_transf, adjacency, node_index, affairs_features, sum_na_per_row = load_data_and_filter_members('../data/abdb-de-all-affairs-'+leg+'-0.csv',\n",
    "                                                                     start_date=from_date, end_date=to_date,\n",
    "                                                                     filter_method='number_NA',cutoff=10,ret_transf=True, delete_links_below=link_cutoff )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: M_0 = |V| = 256 nodes (81 added),|E| = 3726 edges\n",
      "Layer 1: M_1 = |V| = 128 nodes (38 added),|E| = 969 edges\n",
      "Layer 2: M_2 = |V| = 64 nodes (18 added),|E| = 244 edges\n",
      "Layer 3: M_3 = |V| = 32 nodes (7 added),|E| = 65 edges\n",
      "Layer 4: M_4 = |V| = 16 nodes (3 added),|E| = 16 edges\n",
      "Layer 5: M_5 = |V| = 8 nodes (0 added),|E| = 5 edges\n",
      "NN architecture\n",
      "  input: M_0 = 256\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 256 * 2 / 8 = 64\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 2 * 10 = 20\n",
      "    biases: F_1 = 2\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 4 / 4 = 32\n",
      "    weights: F_1 * F_2 * K_2 = 2 * 4 * 10 = 80\n",
      "    biases: F_2 = 4\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 256\n",
      "    weights: M_2 * M_3 = 32 * 256 = 8192\n",
      "    biases: M_3 = 256\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 2\n",
      "    weights: M_3 * M_4 = 256 * 2 = 512\n",
      "    biases: M_4 = 2\n",
      "step 30 / 116 (epoch 2.57 / 10):\n",
      "  learning_rate = 3.24e-02, loss_average = 6.69e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\silus\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  validation accuracy: 61.71 (108 / 175), f1 (weighted): 47.10, loss: 7.01e-01\n",
      "  time: 9s (wall 8s)\n",
      "step 60 / 116 (epoch 5.14 / 10):\n",
      "  learning_rate = 2.36e-02, loss_average = 6.79e-01\n",
      "  validation accuracy: 61.71 (108 / 175), f1 (weighted): 47.10, loss: 6.82e-01\n",
      "  time: 18s (wall 14s)\n",
      "step 90 / 116 (epoch 7.71 / 10):\n",
      "  learning_rate = 1.91e-02, loss_average = 6.81e-01\n",
      "  validation accuracy: 61.71 (108 / 175), f1 (weighted): 47.10, loss: 6.81e-01\n",
      "  time: 24s (wall 20s)\n",
      "step 116 / 116 (epoch 9.94 / 10):\n",
      "  learning_rate = 1.55e-02, loss_average = 6.84e-01\n",
      "  validation accuracy: 61.71 (108 / 175), f1 (weighted): 47.10, loss: 6.85e-01\n",
      "  time: 31s (wall 25s)\n",
      "validation accuracy: peak = 61.71, mean = 61.71\n",
      "NN architecture\n",
      "  input: M_0 = 256\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 256 * 2 / 8 = 64\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 2 * 10 = 20\n",
      "    biases: F_1 = 2\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 4 / 4 = 32\n",
      "    weights: F_1 * F_2 * K_2 = 2 * 4 * 10 = 80\n",
      "    biases: F_2 = 4\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 256\n",
      "    weights: M_2 * M_3 = 32 * 256 = 8192\n",
      "    biases: M_3 = 256\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 2\n",
      "    weights: M_3 * M_4 = 256 * 2 = 512\n",
      "    biases: M_4 = 2\n",
      "step 30 / 116 (epoch 2.57 / 10):\n",
      "  learning_rate = 3.24e-02, loss_average = 6.56e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 6.89e-01\n",
      "  time: 10s (wall 8s)\n",
      "step 60 / 116 (epoch 5.14 / 10):\n",
      "  learning_rate = 2.36e-02, loss_average = 6.68e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 6.90e-01\n",
      "  time: 18s (wall 15s)\n",
      "step 90 / 116 (epoch 7.71 / 10):\n",
      "  learning_rate = 1.91e-02, loss_average = 6.85e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 6.89e-01\n",
      "  time: 25s (wall 21s)\n",
      "step 116 / 116 (epoch 9.94 / 10):\n",
      "  learning_rate = 1.55e-02, loss_average = 6.78e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 6.90e-01\n",
      "  time: 32s (wall 27s)\n",
      "validation accuracy: peak = 60.00, mean = 60.00\n",
      "NN architecture\n",
      "  input: M_0 = 256\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 256 * 2 / 8 = 64\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 2 * 10 = 20\n",
      "    biases: F_1 = 2\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 4 / 4 = 32\n",
      "    weights: F_1 * F_2 * K_2 = 2 * 4 * 10 = 80\n",
      "    biases: F_2 = 4\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 256\n",
      "    weights: M_2 * M_3 = 32 * 256 = 8192\n",
      "    biases: M_3 = 256\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 2\n",
      "    weights: M_3 * M_4 = 256 * 2 = 512\n",
      "    biases: M_4 = 2\n",
      "step 30 / 116 (epoch 2.57 / 10):\n",
      "  learning_rate = 3.24e-02, loss_average = 6.49e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.04e-01\n",
      "  time: 9s (wall 7s)\n",
      "step 60 / 116 (epoch 5.14 / 10):\n",
      "  learning_rate = 2.36e-02, loss_average = 6.73e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.05e-01\n",
      "  time: 17s (wall 14s)\n",
      "step 90 / 116 (epoch 7.71 / 10):\n",
      "  learning_rate = 1.91e-02, loss_average = 6.68e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.08e-01\n",
      "  time: 25s (wall 20s)\n",
      "step 116 / 116 (epoch 9.94 / 10):\n",
      "  learning_rate = 1.55e-02, loss_average = 6.76e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.09e-01\n",
      "  time: 31s (wall 25s)\n",
      "validation accuracy: peak = 56.57, mean = 56.57\n",
      "[[61.71428571 61.71428571]\n",
      " [60.         60.        ]\n",
      " [56.57142857 56.57142857]]\n",
      "IIIII Accuracy: 59.43 (max) 59.43 (mean) Loss: 0.70 (max) 0.69 (mean)\n",
      "NN architecture\n",
      "  input: M_0 = 256\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 256 * 4 / 8 = 128\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 4 * 10 = 40\n",
      "    biases: F_1 = 4\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 8 / 4 = 64\n",
      "    weights: F_1 * F_2 * K_2 = 4 * 8 * 10 = 320\n",
      "    biases: F_2 = 8\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 256\n",
      "    weights: M_2 * M_3 = 64 * 256 = 16384\n",
      "    biases: M_3 = 256\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 2\n",
      "    weights: M_3 * M_4 = 256 * 2 = 512\n",
      "    biases: M_4 = 2\n",
      "step 30 / 116 (epoch 2.57 / 10):\n",
      "  learning_rate = 3.24e-02, loss_average = 6.55e-01\n",
      "  validation accuracy: 61.71 (108 / 175), f1 (weighted): 47.10, loss: 6.82e-01\n",
      "  time: 10s (wall 8s)\n",
      "step 60 / 116 (epoch 5.14 / 10):\n",
      "  learning_rate = 2.36e-02, loss_average = 6.84e-01\n",
      "  validation accuracy: 61.71 (108 / 175), f1 (weighted): 47.10, loss: 6.97e-01\n",
      "  time: 18s (wall 14s)\n",
      "step 90 / 116 (epoch 7.71 / 10):\n",
      "  learning_rate = 1.91e-02, loss_average = 6.84e-01\n",
      "  validation accuracy: 61.71 (108 / 175), f1 (weighted): 47.10, loss: 6.84e-01\n",
      "  time: 26s (wall 20s)\n",
      "step 116 / 116 (epoch 9.94 / 10):\n",
      "  learning_rate = 1.55e-02, loss_average = 6.82e-01\n",
      "  validation accuracy: 61.71 (108 / 175), f1 (weighted): 47.10, loss: 6.85e-01\n",
      "  time: 33s (wall 26s)\n",
      "validation accuracy: peak = 61.71, mean = 61.71\n",
      "NN architecture\n",
      "  input: M_0 = 256\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 256 * 4 / 8 = 128\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 4 * 10 = 40\n",
      "    biases: F_1 = 4\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 8 / 4 = 64\n",
      "    weights: F_1 * F_2 * K_2 = 4 * 8 * 10 = 320\n",
      "    biases: F_2 = 8\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 256\n",
      "    weights: M_2 * M_3 = 64 * 256 = 16384\n",
      "    biases: M_3 = 256\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 2\n",
      "    weights: M_3 * M_4 = 256 * 2 = 512\n",
      "    biases: M_4 = 2\n",
      "step 30 / 116 (epoch 2.57 / 10):\n",
      "  learning_rate = 3.24e-02, loss_average = 6.49e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 6.93e-01\n",
      "  time: 8s (wall 7s)\n",
      "step 60 / 116 (epoch 5.14 / 10):\n",
      "  learning_rate = 2.36e-02, loss_average = 6.77e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 6.94e-01\n",
      "  time: 17s (wall 13s)\n",
      "step 90 / 116 (epoch 7.71 / 10):\n",
      "  learning_rate = 1.91e-02, loss_average = 6.71e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 6.96e-01\n",
      "  time: 22s (wall 17s)\n",
      "step 116 / 116 (epoch 9.94 / 10):\n",
      "  learning_rate = 1.55e-02, loss_average = 6.81e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 6.99e-01\n",
      "  time: 26s (wall 20s)\n",
      "validation accuracy: peak = 60.00, mean = 60.00\n",
      "NN architecture\n",
      "  input: M_0 = 256\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 256 * 4 / 8 = 128\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 4 * 10 = 40\n",
      "    biases: F_1 = 4\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 8 / 4 = 64\n",
      "    weights: F_1 * F_2 * K_2 = 4 * 8 * 10 = 320\n",
      "    biases: F_2 = 8\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 256\n",
      "    weights: M_2 * M_3 = 64 * 256 = 16384\n",
      "    biases: M_3 = 256\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 2\n",
      "    weights: M_3 * M_4 = 256 * 2 = 512\n",
      "    biases: M_4 = 2\n",
      "step 30 / 116 (epoch 2.57 / 10):\n",
      "  learning_rate = 3.24e-02, loss_average = 6.51e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.05e-01\n",
      "  time: 6s (wall 5s)\n",
      "step 60 / 116 (epoch 5.14 / 10):\n",
      "  learning_rate = 2.36e-02, loss_average = 6.73e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.19e-01\n",
      "  time: 12s (wall 9s)\n",
      "step 90 / 116 (epoch 7.71 / 10):\n",
      "  learning_rate = 1.91e-02, loss_average = 6.78e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.07e-01\n",
      "  time: 17s (wall 13s)\n",
      "step 116 / 116 (epoch 9.94 / 10):\n",
      "  learning_rate = 1.55e-02, loss_average = 6.68e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.08e-01\n",
      "  time: 21s (wall 16s)\n",
      "validation accuracy: peak = 56.57, mean = 56.57\n",
      "[[61.71428571 61.71428571]\n",
      " [60.         60.        ]\n",
      " [56.57142857 56.57142857]]\n",
      "IIIII Accuracy: 59.43 (max) 59.43 (mean) Loss: 0.71 (max) 0.70 (mean)\n",
      "NN architecture\n",
      "  input: M_0 = 256\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 256 * 6 / 8 = 192\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 6 * 10 = 60\n",
      "    biases: F_1 = 6\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 12 / 4 = 96\n",
      "    weights: F_1 * F_2 * K_2 = 6 * 12 * 10 = 720\n",
      "    biases: F_2 = 12\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 256\n",
      "    weights: M_2 * M_3 = 96 * 256 = 24576\n",
      "    biases: M_3 = 256\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 2\n",
      "    weights: M_3 * M_4 = 256 * 2 = 512\n",
      "    biases: M_4 = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 30 / 116 (epoch 2.57 / 10):\n",
      "  learning_rate = 3.24e-02, loss_average = 6.53e-01\n",
      "  validation accuracy: 61.71 (108 / 175), f1 (weighted): 47.10, loss: 6.89e-01\n",
      "  time: 10s (wall 7s)\n",
      "step 60 / 116 (epoch 5.14 / 10):\n",
      "  learning_rate = 2.36e-02, loss_average = 6.79e-01\n",
      "  validation accuracy: 61.71 (108 / 175), f1 (weighted): 47.10, loss: 6.89e-01\n",
      "  time: 18s (wall 14s)\n",
      "step 90 / 116 (epoch 7.71 / 10):\n",
      "  learning_rate = 1.91e-02, loss_average = 6.81e-01\n",
      "  validation accuracy: 61.71 (108 / 175), f1 (weighted): 47.10, loss: 6.90e-01\n",
      "  time: 25s (wall 19s)\n",
      "step 116 / 116 (epoch 9.94 / 10):\n",
      "  learning_rate = 1.55e-02, loss_average = 6.79e-01\n",
      "  validation accuracy: 61.71 (108 / 175), f1 (weighted): 47.10, loss: 6.89e-01\n",
      "  time: 32s (wall 24s)\n",
      "validation accuracy: peak = 61.71, mean = 61.71\n",
      "NN architecture\n",
      "  input: M_0 = 256\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 256 * 6 / 8 = 192\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 6 * 10 = 60\n",
      "    biases: F_1 = 6\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 12 / 4 = 96\n",
      "    weights: F_1 * F_2 * K_2 = 6 * 12 * 10 = 720\n",
      "    biases: F_2 = 12\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 256\n",
      "    weights: M_2 * M_3 = 96 * 256 = 24576\n",
      "    biases: M_3 = 256\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 2\n",
      "    weights: M_3 * M_4 = 256 * 2 = 512\n",
      "    biases: M_4 = 2\n",
      "step 30 / 116 (epoch 2.57 / 10):\n",
      "  learning_rate = 3.24e-02, loss_average = 6.56e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 6.97e-01\n",
      "  time: 9s (wall 7s)\n",
      "step 60 / 116 (epoch 5.14 / 10):\n",
      "  learning_rate = 2.36e-02, loss_average = 6.87e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 6.95e-01\n",
      "  time: 17s (wall 13s)\n",
      "step 90 / 116 (epoch 7.71 / 10):\n",
      "  learning_rate = 1.91e-02, loss_average = 6.74e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 6.98e-01\n",
      "  time: 24s (wall 19s)\n",
      "step 116 / 116 (epoch 9.94 / 10):\n",
      "  learning_rate = 1.55e-02, loss_average = 6.85e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 7.01e-01\n",
      "  time: 30s (wall 23s)\n",
      "validation accuracy: peak = 60.00, mean = 60.00\n",
      "NN architecture\n",
      "  input: M_0 = 256\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 256 * 6 / 8 = 192\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 6 * 10 = 60\n",
      "    biases: F_1 = 6\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 12 / 4 = 96\n",
      "    weights: F_1 * F_2 * K_2 = 6 * 12 * 10 = 720\n",
      "    biases: F_2 = 12\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 256\n",
      "    weights: M_2 * M_3 = 96 * 256 = 24576\n",
      "    biases: M_3 = 256\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 2\n",
      "    weights: M_3 * M_4 = 256 * 2 = 512\n",
      "    biases: M_4 = 2\n",
      "step 30 / 116 (epoch 2.57 / 10):\n",
      "  learning_rate = 3.24e-02, loss_average = 6.58e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.05e-01\n",
      "  time: 9s (wall 7s)\n",
      "step 60 / 116 (epoch 5.14 / 10):\n",
      "  learning_rate = 2.36e-02, loss_average = 6.76e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.05e-01\n",
      "  time: 16s (wall 12s)\n",
      "step 90 / 116 (epoch 7.71 / 10):\n",
      "  learning_rate = 1.91e-02, loss_average = 6.74e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.04e-01\n",
      "  time: 23s (wall 18s)\n",
      "step 116 / 116 (epoch 9.94 / 10):\n",
      "  learning_rate = 1.55e-02, loss_average = 6.69e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.06e-01\n",
      "  time: 27s (wall 21s)\n",
      "validation accuracy: peak = 56.57, mean = 56.57\n",
      "[[61.71428571 61.71428571]\n",
      " [60.         60.        ]\n",
      " [56.57142857 56.57142857]]\n",
      "IIIII Accuracy: 59.43 (max) 59.43 (mean) Loss: 0.70 (max) 0.70 (mean)\n",
      "NN architecture\n",
      "  input: M_0 = 256\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 256 * 8 / 8 = 256\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 8 * 10 = 80\n",
      "    biases: F_1 = 8\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 16 / 4 = 128\n",
      "    weights: F_1 * F_2 * K_2 = 8 * 16 * 10 = 1280\n",
      "    biases: F_2 = 16\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 256\n",
      "    weights: M_2 * M_3 = 128 * 256 = 32768\n",
      "    biases: M_3 = 256\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 2\n",
      "    weights: M_3 * M_4 = 256 * 2 = 512\n",
      "    biases: M_4 = 2\n",
      "step 30 / 116 (epoch 2.57 / 10):\n",
      "  learning_rate = 3.24e-02, loss_average = 6.63e-01\n",
      "  validation accuracy: 61.71 (108 / 175), f1 (weighted): 47.10, loss: 6.90e-01\n",
      "  time: 8s (wall 5s)\n",
      "step 60 / 116 (epoch 5.14 / 10):\n",
      "  learning_rate = 2.36e-02, loss_average = 6.85e-01\n",
      "  validation accuracy: 62.29 (109 / 175), f1 (weighted): 49.36, loss: 6.97e-01\n",
      "  time: 14s (wall 9s)\n",
      "step 90 / 116 (epoch 7.71 / 10):\n",
      "  learning_rate = 1.91e-02, loss_average = 6.86e-01\n",
      "  validation accuracy: 61.14 (107 / 175), f1 (weighted): 51.19, loss: 7.00e-01\n",
      "  time: 19s (wall 13s)\n",
      "step 116 / 116 (epoch 9.94 / 10):\n",
      "  learning_rate = 1.55e-02, loss_average = 6.81e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 49.76, loss: 6.97e-01\n",
      "  time: 24s (wall 16s)\n",
      "validation accuracy: peak = 62.29, mean = 61.29\n",
      "NN architecture\n",
      "  input: M_0 = 256\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 256 * 8 / 8 = 256\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 8 * 10 = 80\n",
      "    biases: F_1 = 8\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 16 / 4 = 128\n",
      "    weights: F_1 * F_2 * K_2 = 8 * 16 * 10 = 1280\n",
      "    biases: F_2 = 16\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 256\n",
      "    weights: M_2 * M_3 = 128 * 256 = 32768\n",
      "    biases: M_3 = 256\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 2\n",
      "    weights: M_3 * M_4 = 256 * 2 = 512\n",
      "    biases: M_4 = 2\n",
      "step 30 / 116 (epoch 2.57 / 10):\n",
      "  learning_rate = 3.24e-02, loss_average = 6.47e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 7.00e-01\n",
      "  time: 6s (wall 5s)\n",
      "step 60 / 116 (epoch 5.14 / 10):\n",
      "  learning_rate = 2.36e-02, loss_average = 6.83e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 7.07e-01\n",
      "  time: 12s (wall 9s)\n",
      "step 90 / 116 (epoch 7.71 / 10):\n",
      "  learning_rate = 1.91e-02, loss_average = 6.91e-01\n",
      "  validation accuracy: 58.29 (102 / 175), f1 (weighted): 45.14, loss: 7.09e-01\n",
      "  time: 18s (wall 13s)\n",
      "step 116 / 116 (epoch 9.94 / 10):\n",
      "  learning_rate = 1.55e-02, loss_average = 6.82e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 7.04e-01\n",
      "  time: 23s (wall 16s)\n",
      "validation accuracy: peak = 60.00, mean = 59.57\n",
      "NN architecture\n",
      "  input: M_0 = 256\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 256 * 8 / 8 = 256\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 8 * 10 = 80\n",
      "    biases: F_1 = 8\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 16 / 4 = 128\n",
      "    weights: F_1 * F_2 * K_2 = 8 * 16 * 10 = 1280\n",
      "    biases: F_2 = 16\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 256\n",
      "    weights: M_2 * M_3 = 128 * 256 = 32768\n",
      "    biases: M_3 = 256\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 2\n",
      "    weights: M_3 * M_4 = 256 * 2 = 512\n",
      "    biases: M_4 = 2\n",
      "step 30 / 116 (epoch 2.57 / 10):\n",
      "  learning_rate = 3.24e-02, loss_average = 6.47e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.09e-01\n",
      "  time: 7s (wall 5s)\n",
      "step 60 / 116 (epoch 5.14 / 10):\n",
      "  learning_rate = 2.36e-02, loss_average = 6.73e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.18e-01\n",
      "  time: 13s (wall 9s)\n",
      "step 90 / 116 (epoch 7.71 / 10):\n",
      "  learning_rate = 1.91e-02, loss_average = 6.76e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.11e-01\n",
      "  time: 18s (wall 12s)\n",
      "step 116 / 116 (epoch 9.94 / 10):\n",
      "  learning_rate = 1.55e-02, loss_average = 6.67e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.15e-01\n",
      "  time: 23s (wall 16s)\n",
      "validation accuracy: peak = 56.57, mean = 56.57\n",
      "[[62.28571429 61.28571429]\n",
      " [60.         59.57142857]\n",
      " [56.57142857 56.57142857]]\n",
      "IIIII Accuracy: 59.62 (max) 59.14 (mean) Loss: 0.71 (max) 0.70 (mean)\n",
      "NN architecture\n",
      "  input: M_0 = 256\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 256 * 16 / 8 = 512\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 16 * 10 = 160\n",
      "    biases: F_1 = 16\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 32 / 4 = 256\n",
      "    weights: F_1 * F_2 * K_2 = 16 * 32 * 10 = 5120\n",
      "    biases: F_2 = 32\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 256\n",
      "    weights: M_2 * M_3 = 256 * 256 = 65536\n",
      "    biases: M_3 = 256\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 2\n",
      "    weights: M_3 * M_4 = 256 * 2 = 512\n",
      "    biases: M_4 = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 30 / 116 (epoch 2.57 / 10):\n",
      "  learning_rate = 3.24e-02, loss_average = 6.66e-01\n",
      "  validation accuracy: 53.71 (94 / 175), f1 (weighted): 51.76, loss: 7.13e-01\n",
      "  time: 8s (wall 5s)\n",
      "step 60 / 116 (epoch 5.14 / 10):\n",
      "  learning_rate = 2.36e-02, loss_average = 6.81e-01\n",
      "  validation accuracy: 61.14 (107 / 175), f1 (weighted): 47.82, loss: 6.91e-01\n",
      "  time: 15s (wall 10s)\n",
      "step 90 / 116 (epoch 7.71 / 10):\n",
      "  learning_rate = 1.91e-02, loss_average = 6.76e-01\n",
      "  validation accuracy: 61.71 (108 / 175), f1 (weighted): 47.10, loss: 6.95e-01\n",
      "  time: 22s (wall 14s)\n",
      "step 116 / 116 (epoch 9.94 / 10):\n",
      "  learning_rate = 1.55e-02, loss_average = 6.79e-01\n",
      "  validation accuracy: 62.29 (109 / 175), f1 (weighted): 51.89, loss: 6.92e-01\n",
      "  time: 29s (wall 17s)\n",
      "validation accuracy: peak = 62.29, mean = 59.71\n",
      "NN architecture\n",
      "  input: M_0 = 256\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 256 * 16 / 8 = 512\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 16 * 10 = 160\n",
      "    biases: F_1 = 16\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 32 / 4 = 256\n",
      "    weights: F_1 * F_2 * K_2 = 16 * 32 * 10 = 5120\n",
      "    biases: F_2 = 32\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 256\n",
      "    weights: M_2 * M_3 = 256 * 256 = 65536\n",
      "    biases: M_3 = 256\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 2\n",
      "    weights: M_3 * M_4 = 256 * 2 = 512\n",
      "    biases: M_4 = 2\n",
      "step 30 / 116 (epoch 2.57 / 10):\n",
      "  learning_rate = 3.24e-02, loss_average = 6.58e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 7.04e-01\n",
      "  time: 8s (wall 5s)\n",
      "step 60 / 116 (epoch 5.14 / 10):\n",
      "  learning_rate = 2.36e-02, loss_average = 6.88e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 7.01e-01\n",
      "  time: 14s (wall 9s)\n",
      "step 90 / 116 (epoch 7.71 / 10):\n",
      "  learning_rate = 1.91e-02, loss_average = 6.79e-01\n",
      "  validation accuracy: 57.71 (101 / 175), f1 (weighted): 46.56, loss: 7.02e-01\n",
      "  time: 21s (wall 13s)\n",
      "step 116 / 116 (epoch 9.94 / 10):\n",
      "  learning_rate = 1.55e-02, loss_average = 6.79e-01\n",
      "  validation accuracy: 60.00 (105 / 175), f1 (weighted): 45.00, loss: 7.00e-01\n",
      "  time: 27s (wall 17s)\n",
      "validation accuracy: peak = 60.00, mean = 59.43\n",
      "NN architecture\n",
      "  input: M_0 = 256\n",
      "  layer 1: cgconv1\n",
      "    representation: M_0 * F_1 / p_1 = 256 * 16 / 8 = 512\n",
      "    weights: F_0 * F_1 * K_1 = 1 * 16 * 10 = 160\n",
      "    biases: F_1 = 16\n",
      "  layer 2: cgconv2\n",
      "    representation: M_1 * F_2 / p_2 = 32 * 32 / 4 = 256\n",
      "    weights: F_1 * F_2 * K_2 = 16 * 32 * 10 = 5120\n",
      "    biases: F_2 = 32\n",
      "  layer 3: fc1\n",
      "    representation: M_3 = 256\n",
      "    weights: M_2 * M_3 = 256 * 256 = 65536\n",
      "    biases: M_3 = 256\n",
      "  layer 4: logits (softmax)\n",
      "    representation: M_4 = 2\n",
      "    weights: M_3 * M_4 = 256 * 2 = 512\n",
      "    biases: M_4 = 2\n",
      "step 30 / 116 (epoch 2.57 / 10):\n",
      "  learning_rate = 3.24e-02, loss_average = 6.68e-01\n",
      "  validation accuracy: 44.57 (78 / 175), f1 (weighted): 44.57, loss: 7.37e-01\n",
      "  time: 8s (wall 5s)\n",
      "step 60 / 116 (epoch 5.14 / 10):\n",
      "  learning_rate = 2.36e-02, loss_average = 6.85e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.33e-01\n",
      "  time: 16s (wall 10s)\n",
      "step 90 / 116 (epoch 7.71 / 10):\n",
      "  learning_rate = 1.91e-02, loss_average = 6.77e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.16e-01\n",
      "  time: 23s (wall 14s)\n",
      "step 116 / 116 (epoch 9.94 / 10):\n",
      "  learning_rate = 1.55e-02, loss_average = 6.69e-01\n",
      "  validation accuracy: 56.57 (99 / 175), f1 (weighted): 40.88, loss: 7.27e-01\n",
      "  time: 29s (wall 18s)\n",
      "validation accuracy: peak = 56.57, mean = 53.57\n",
      "[[62.28571429 59.71428571]\n",
      " [60.         59.42857143]\n",
      " [56.57142857 53.57142857]]\n",
      "IIIII Accuracy: 59.62 (max) 57.57 (mean) Loss: 0.72 (max) 0.71 (mean)\n"
     ]
    }
   ],
   "source": [
    "hyperparam_values = np.array([2,4,6,8,16])\n",
    "hyperparam_name = 'F'\n",
    "k_fold = 3\n",
    "nn_accuracy, nn_loss = cross_validate_convNN(X_train, y_train, adjacency, hyperparam_name, hyperparam_values, k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([59.42857143, 59.42857143]),\n",
       " array([59.42857143, 59.42857143]),\n",
       " array([59.42857143, 59.42857143]),\n",
       " array([59.61904762, 59.14285714]),\n",
       " array([59.61904762, 57.57142857])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-6abd1a267736>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyperparam_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtwinx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0max2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyperparam_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'g.-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'g'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(15, 5))\n",
    "ax1.plot(hyperparam_values, np.array(nn_accuracy)[:,1], 'b.-')\n",
    "ax1.set_ylabel('Accuracy', color='b')\n",
    "ax1.set_xlabel(hyperparam_name)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(hyperparam_values, nn_loss[:,1], 'g.-')\n",
    "ax2.set_ylabel('training loss', color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0709271430969238,\n",
       " 1.1210624490465437,\n",
       " 1.055032730102539,\n",
       " 1.0497947760990687,\n",
       " 1.0873257092067175,\n",
       " 1.168379511151995,\n",
       " 1.0576768262045724,\n",
       " array([69.33615855, 68.16202093])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
